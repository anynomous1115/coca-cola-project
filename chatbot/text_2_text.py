import os
import warnings
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from fastapi import FastAPI
from pydantic import BaseModel
import uuid
from chatbot.order import Order


warnings.filterwarnings("ignore")
load_dotenv()

# --- API Keys & Clients ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_HOST = os.getenv("QDRANT_HOST")

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("models/gemini-2.0-flash")

# Configure Qdrant
qdrant_client = QdrantClient(api_key=QDRANT_API_KEY, url=QDRANT_HOST, https=True)
COLLECTION_NAME = ["coca_cola_docs", "coca_cola_products"]

# Initialize FastAPI
app = FastAPI()

# Schema
class QuestionRequest(BaseModel):
    session_id: str
    question: str

# Embedding model
embedding_model = SentenceTransformer("intfloat/multilingual-e5-large-instruct")

# Store multi-turn conversations
chat_memory = {}
conversation_state = {}

# Multi-turn order management
def handle_order_conversation(session_id: str, question: str) -> str:
    state = conversation_state.get(session_id, {
        "intent": "ƒë·∫∑t h√†ng",
        "step": "ask_name",
        "data": {}
    })

    step = state["step"]
    data = state["data"]

    if step == "ask_name":
        state["data"]["name"] = question
        state["step"] = "ask_phone"
        conversation_state[session_id] = state
        return "Vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i c·ªßa b·∫°n."

    elif step == "ask_phone":
        state["data"]["phone"] = question
        state["step"] = "ask_address"
        conversation_state[session_id] = state
        return "B·∫°n vui l√≤ng cho bi·∫øt ƒë·ªãa ch·ªâ nh·∫≠n h√†ng?"

    elif step == "ask_address":
        state["data"]["address"] = question
        state["step"] = "ask_product"
        conversation_state[session_id] = state
        return "B·∫°n mu·ªën ƒë·∫∑t s·∫£n ph·∫©m g√¨? (VD: CocaCola Lon x2)"

    elif step == "ask_product":
        # Parse product
        import re
        match = re.match(r"(.*) x(\d+)", question.strip())
        if not match:
            return "Vui l√≤ng nh·∫≠p ƒë√∫ng ƒë·ªãnh d·∫°ng: T√™n s·∫£n ph·∫©m xS·ªë l∆∞·ª£ng (VD: CocaCola Lon x2)"

        product_name = match.group(1).strip()
        quantity = int(match.group(2))
        price = 1000  
        total = price * quantity

        item = {
            "product": "product123",
            "productName": product_name,
            "quantity": quantity,
            "price": price,
            "total": total
        }

        # Create order
        totalAmount = total
        order = Order(
            orderId=str(uuid.uuid4()),
            user="user123",
            items=[item],
            totalAmount=totalAmount,
            customerInfo={
                "name": data["name"],
                "phone": data["phone"],
                "address": data["address"]
            },
            status="pending"
        )

        # orders_collection.insert_one(order.dict())
        conversation_state.pop(session_id, None)  # reset state

        return f"üéâ ƒê∆°n h√†ng c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\nM√£ ƒë∆°n: {order.orderId}"

    return "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói trong qu√° tr√¨nh ƒë·∫∑t h√†ng."

# ----------- Modules -----------

def get_context(question, k=3):
    embedding = embedding_model.encode(f"Documents for retrieving: {question}").tolist()
    all_contexts = []
    for collection in COLLECTION_NAME:
        response = qdrant_client.search(
            collection_name=collection,
            query_vector=embedding,
            limit=k,
            with_payload=True
        )
        contexts = [hit.payload for hit in response]
        all_contexts.extend(contexts)
    return all_contexts

def detect_intent(question: str) -> str:
    prompt = (
        "X√°c ƒë·ªãnh √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi sau. "
        "C√°c √Ω ƒë·ªãnh c√≥ th·ªÉ l√†: h·ªèi s·∫£n ph·∫©m, h·ªèi t·ªìn kho, h·ªèi khuy·∫øn m√£i, khi·∫øu n·∫°i, ch√†o h·ªèi, ƒë·∫∑t h√†ng, kh√°c.\n"
        f"C√¢u h·ªèi: {question}\n√ù ƒë·ªãnh:"
    )
    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip().lower()
    except:
        return "kh√°c"

def analyze_sentiment(question: str) -> str:
    prompt = (
        "Ph√¢n t√≠ch c·∫£m x√∫c c·ªßa c√¢u sau. "
        "C·∫£m x√∫c c√≥ th·ªÉ l√†: t√≠ch c·ª±c, ti√™u c·ª±c, trung l·∫≠p.\n"
        f"C√¢u: {question}\nC·∫£m x√∫c:"
    )
    try:
        response = gemini_model.generate_content(prompt)
        return response.text.strip().lower()
    except:
        return "trung l·∫≠p"

def generate_answer(session_id: str, question: str) -> str:
    context = get_context(question)
    intent = detect_intent(question)
    sentiment = analyze_sentiment(question)

    # Get chat history
    history = chat_memory.get(session_id, [])
    history_text = "\n".join([f"Ng∆∞·ªùi d√πng: {q}\nTr·ª£ l√Ω: {a}" for q, a in history])

    # Check if it's a conversation in progress
    if session_id in conversation_state:
        # Ch·ªâ x·ª≠ l√Ω theo ti·∫øn tr√¨nh ƒë·∫∑t h√†ng
        state = conversation_state[session_id]
        if state["intent"] == "ƒë·∫∑t h√†ng":
            return handle_order_conversation(session_id, question)
    
    if intent == "ƒë·∫∑t h√†ng":
        conversation_state[session_id] = {
            "intent": "ƒë·∫∑t h√†ng",
            "step": "ask_name",
            "data": {}
        }
        return "B·∫°n mu·ªën ƒë·∫∑t h√†ng ? Vui l√≤ng cung c·∫•p t√™n c·ªßa b·∫°n ƒë·ªÉ b·∫Øt ƒë·∫ßu."

    else:
        # Prompt 
        prompt = (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh v√† th√¢n thi·ªán. "
            "Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n ng·ªØ c·∫£nh v√† gi·ªØ m·∫°ch h·ªôi tho·∫°i.\n"
            f"√ù ƒë·ªãnh ng∆∞·ªùi d√πng: {intent}\n"
            f"C·∫£m x√∫c ng∆∞·ªùi d√πng: {sentiment}\n"
            f"L·ªãch s·ª≠ tr√≤ chuy·ªán:\n{history_text}\n\n"
            f"Ng·ªØ c·∫£nh:\n{context}\n\n"
            f"Ng∆∞·ªùi d√πng: {question}\nTr·ª£ l√Ω:"
        )

        try:
            response = gemini_model.generate_content(prompt)
            answer = response.text.strip()
            # Save to chat history
            chat_memory.setdefault(session_id, []).append((question, answer))
            return answer
        except Exception as e:
            return f"ƒê√£ x·∫£y ra l·ªói: {e}"

# ----------- Endpoints -----------

@app.get("/greet")
def greet_user():
    return {
        "message": "Xin ch√†o, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?",
        "suggestions": ["H·ªèi v·ªÅ s·∫£n ph·∫©m", "T·ªìn kho hi·ªán t·∫°i", "Khuy·∫øn m√£i h√¥m nay"]
    }

@app.post("/ask")
def ask_question(data: QuestionRequest):
    question = data.question.strip()
    session_id = data.session_id.strip()

    if not question:
        return {"error": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng."}
    if not session_id:
        return {"error": "Thi·∫øu session_id ƒë·ªÉ l∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i."}

    answer = generate_answer(session_id, question)
    return {
        "session_id": session_id,
        "question": question,
        "answer": answer,
        "history": chat_memory.get(session_id, [])
    }
